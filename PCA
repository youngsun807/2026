PCA(Principal Component Analysis, 주성분 분석)
중요한 정보는 최대한 유지하면서 데이터 차원 줄이기

📚 실생활 비유로 이해하기
예시 1: 학생 성적표
학생의 성적을 평가할 때:

원래 데이터: 수학, 물리, 화학, 생물, 국어, 영어, 역사, 지리 (8개 과목)
PCA 적용 후: "이과 실력", "문과 실력" (2개 종합 점수)
8개 과목 점수 대신 2개의 종합 점수로 학생을 평가할 수 있게 되는 거죠!

예시 2: 사진 압축
원래: 1000x1000 픽셀 (100만 개의 정보)
PCA 적용: 중요한 특징 100개로 압축
결과: 용량은 줄었지만 사진의 핵심 내용은 유지


❕❕❕❕PCA 필요 이유
차원의 저주 해결
변수(차원, 특성)이 too much → 모델 복잡, 학습 어려움
시각화
3차원 이상인 경우 시각적 그래프로 확인 어려움 → PCA로 2D, 3D로 축소하면 데이터를 그래프로 볼 수 있음
노이즈 제거
중요하지 않은 정보(노이즈) 제거 가능
계산속도 향상
데이터 간략화 → 모델 학습 속도 향상
💻PCA 예시코드
from sklearn.decomposition import PCA
import numpy as np
 
# 예시: 10개 특성을 가진 데이터
X = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
              [2, 4, 6, 8, 10, 12, 14, 16, 18, 20],
              [3, 6, 9, 12, 15, 18, 21, 24, 27, 30],
              [4, 8, 12, 16, 20, 24, 28, 32, 36, 40]
              ])
 
# PCA 모델 생성 (10개 → 2개로 줄이기)
pca = PCA(n_components=2) # 개수 지정
#pca = PCA(n_components=0.95) 전체 정보의 95%을 유지하는만큼 압축
#pca = PCA() 모든 주성분
 
# PCA 적용
X_reduced = pca.fit_transform(X) # pca.fit(X) + (X_reducted = pca.trasform(X)) 2개의 코드를 합친것 -> 기출문제로 출제 패턴 분석하고 공부
 
# 결과 확인
print(f"원래 차원: {X.shape}")  # (샘플 수, 10)
print(f"축소 후: {X_reduced.shape}")  # (샘플 수, 2)
print(f"보존된 정보 비율: {pca.explained_variance_ratio_.sum():.2%}")
 
 
-----------------------------------------------------------------
<결과값>
원래 차원: (4, 10)
축소 후: (4, 2)
보존된 정보 비율: 100.00%
pca.explained_variance_ratio_.sum()
각 주성분이 설명하는 분산의 비율합 → 전체 정보의 몇%를 보존하는지 알기 위해!

# 50개 주성분 사용
pca = PCA(n_components=50)
pca.fit(X_train)
 
# 각 주성분의 설명 비율
print(pca.explained_variance_ratio_)
# [0.15, 0.12, 0.10, 0.08, 0.07, ...]  (50개)
 
# 모두 합치면?
total = pca.explained_variance_ratio_.sum()
print(total)  # 0.952 (95.2%)
 
------------------------------------
PC1: 15% 정보 설명
PC2: 12% 정보 설명
PC3: 10% 정보 설명
...
PC50: 0.01% 정보 설명
━━━━━━━━━━━━━━━━━━━━
합계: 95.2% 정보 설명 ← 이게 중요!
784개 중 734개 특성을 버렸지만, 정보 손실은 4.8%밖에 안됨
→ "50개 주성분으로 원본 정보의 95.2%를 보존했다!"
from sklearn.decomposition import PCA
import numpy as np
 
# 예시 데이터 (5개 샘플, 4개 특성)
X_train = np.array([
    [1, 2, 3, 4],
    [2, 3, 4, 5],
    [3, 4, 5, 6],
    [4, 5, 6, 7],
    [5, 6, 7, 8]
])
 
X_test = np.array([
    [1.5, 2.5, 3.5, 4.5],
    [2.5, 3.5, 4.5, 5.5]
])
 
print("원본 X_train 크기:", X_train.shape)  # (5, 4)
print("원본 X_test 크기:", X_test.shape)    # (2, 4)
 
# 1. PCA 모델 생성
pca = PCA(n_components=2)  # 4차원 → 2차원으로 축소
 
# 2. 훈련 데이터로 학습 + 변환
X_train_pca = pca.fit_transform(X_train)
print("\n변환된 X_train_pca 크기:", X_train_pca.shape)  # (5, 2)
print(X_train_pca)
# [[-2.828  0.   ]
#  [-1.414  0.   ]
#  [ 0.     0.   ]
#  [ 1.414  0.   ]
#  [ 2.828  0.   ]]
 
# 3. 테스트 데이터는 변환만
X_test_pca = pca.transform(X_test)
print("\n변환된 X_test_pca 크기:", X_test_pca.shape)  # (2, 2)
print(X_test_pca)
# [[-2.121  0.   ]
#  [-0.707  0.   ]]
 
# 학습된 정보 확인
print("\n각 주성분이 설명하는 분산 비율:")
print(pca.explained_variance_ratio_)  # [1.0, 0.0] → 첫 번째 주성분이 100% 설명
 
print("\n원본 특성들의 평균값 (학습 시 저장됨):")
print(pca.mean_)  # [3. 4. 5. 6.]
pca.explained_variance_ratio_
각 주성분이 설명하는 분산(정보)의 비율

# 예시: 50개 주성분을 사용한 경우
pca.explained_variance_ratio_
 
# 결과 (예시):
array([0.15, 0.12, 0.10, 0.08, 0.07, 0.05, 0.04, 0.03, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.01, ...])
#      ↑     ↑     ↑     ↑
#     PC1   PC2   PC3   PC4
 
#0.15(15%) -> 1번째 주성분이 전체 정보의 15%를 설명
#0.12(12%) -> 2번째 주성분이 전체 정보의 12%를 설명
....
fit(x_train) : 학습

"이 학생들의 성적 패턴을 분석해서 '이과 실력', '문과 실력' 계산법을 만들어라"
x_train 데이터 분석 후 주성분 탐색
학습 대상
각 특성의 평균
데이터 공분산 행렬
주성분 방향(고유벡터)
각 주성분 중요도(고유값)
학습 후 저장되는 정보들
pca.mean_ = 각 특성의 평균값
pca.components_ = 주성분벡터들(50개)
pca.explained_variance_ = 각 주성분의 설명 분산
transform(x_train) : 변환

"계산법을 사용해서 각 학생의 '이과 실력', '문과 실력' 점수를 실제로 계산해라"
훈련 데이터로 배운 규칙을 적용
배운 내용을 실전 시험에 적용!
학습한 주성분들을 사용해서 데이터를 실제로 변환

784 → 50차원으로 축소
x_test_pca = pca.transform(x_test)

테스트 데이터 변환만 수행(학습X)
fit_transform → 테스트 데이터로 다시 학습? 부정행위 느낌!
테스트 데이터 = 미래에 볼 데이터 → 훈련 데이터로 배운 변환 규칙을 테스트 데이터에도 똑같이 적용 必(주성분방향, 평균값 등이 달라지면 안됨)
model2.score(x_test_pca, y_test)
모델 성능 평가 코드
score = 모델 정확도 계산 메서드

score 메서드 내부코드 수동 구현
# score() 메서드 내부 동작 (간소화 버전)
 
# 1단계: 예측
predictions = model2.predict(X_test_pca)
# → 모델이 각 테스트 이미지가 무슨 숫자인지 예측
 
# 2단계: 정답과 비교
correct = (predictions == y_test)
# → 예측이 정답과 같은지 확인 (True/False 배열)
 
# 3단계: 정확도 계산
accuracy = correct.sum() / len(y_test)
# → 맞춘 개수 / 전체 개수
 
# 4단계: 결과 반환
return accuracy  # 예: 0.965 (96.5%)

🧨장단점
장점
차원 축소로 계산 효율 증가
시각화 가능
과적합 방지
방지 이유1: PCA는 노이즈 제거
방지 이유2: 모델 복잡도 높을수록 과적합이 쉬워짐 → PCA는 차원 축소로 모델 복잡도 감소
방지 이유3: 다중공선성(multicollinearity)이 있으면 작은 데이터 변화에 모델이 크게 흔들림 → PCA는 주성분으로 압축하여 모델 안정화 기여 
다중공선성 : 변수들끼리 서로 상관관계가 높은 현상
단점
해석이 어려움 → "제1주성분"이 실제로 무엇을 의미하는지 알기 어려움
선형 관계만 파악 가능 : 복잡한 비선형 패턴 파악 불가 가능
스케일링 필수 : 변수들 단위가 다를 시, 반드시 정규화 必

✨ 실무 사용 예시
얼굴 인식 : 수천개 픽셀 → 주요 얼굴 특징으로 압축
추천 시스템 : 사용자 다양한 행동 → 핵심 선호도로 요약
유전자 분석 : 수만개 유전자 → 주요 유전적 패턴 파악
이미지 압축 : jpeg 등에서 사용

🔔 시작방법
데이터 정규화(StandardScaler)
설명된 분산 비율 확인(보통 80~95% 유지)
시각화로 결과 확인
원본 데이터와 비교
